{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.dataset import read_lang_dataset, tokenize_dataset, tokenize_document, get_vocab_mapping, gateways\n",
    "\n",
    "sns.set_theme()\n",
    "sns.set(rc={'figure.figsize': (12, 8)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_lang_dataset('data/dataset_github_codes.db')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cache = True\n",
    "# load \"tokenized_sample.pkl\" from disk if it exists\n",
    "if use_cache and os.path.exists(\"models/tokenized_sample.pkl\"):\n",
    "    tokenized_sample = pd.read_pickle(\"models/tokenized_sample.pkl\")\n",
    "else:\n",
    "    tokenized_sample = tokenize_dataset(dataset)\n",
    "    tokenized_sample.to_pickle(\"models/tokenized_sample.pkl\")\n",
    "\n",
    "tokenized_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lang_model(dataset, complete_model, language):\n",
    "    model = Word2Vec.load(\"models/complete_model.gensim\")\n",
    "    #model = FastText.load(\"models/complete_model.gensim\")\n",
    "    #model = Word2Vec(vector_size=100, window=10, min_count=10, workers=4)\n",
    "    #model.reset_from(complete_model)\n",
    "    #model.init_weights()\n",
    "    train_dataset = dataset.code[dataset.language == language]\n",
    "    model.train(train_dataset, total_examples=len(train_dataset), epochs=5)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_keywords = [\"if\", \"else\", \"for\", \"while\", \"class\", \"struct\", \"return\", \"continue\", \"break\", \"var\", \"try\", \"catch\", \"except\"]\n",
    "semantic_symbols = [\"{\", \"}\", \":\"]\n",
    "\n",
    "language_specific_words = common_keywords + semantic_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "def add_language_prefix(code, language):\n",
    "    return [f\"{language}_{keyword}\" if keyword in language_specific_words else keyword for keyword in code]\n",
    "\n",
    "for language in tokenized_sample.language.unique():\n",
    "    tokenized_sample.loc[tokenized_sample.language == language, \"code\"] = tokenized_sample[tokenized_sample.language == language].code.apply(partial(add_language_prefix, language=language))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete_model = FastText(vector_size=100, window=5, min_count=5, workers=8)\n",
    "#complete_model = Word2Vec(vector_size=500, window=5, min_count=10, workers=8)\n",
    "complete_model = FastText(vector_size=100, window=5, min_count=10, workers=8)\n",
    "complete_model.build_vocab(tokenized_sample.code)\n",
    "complete_model.train(tokenized_sample.code, total_examples=complete_model.corpus_count, epochs=5)\n",
    "complete_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inserted_tokens = [\"INT_LITERAL\", \"FLOAT_LITERAL\", \"STRING_LITERAL\", \"VARIABLE\"]\n",
    "math_ops = [\"+\", \"-\", \"*\", \"/\", \"%\", \"&&\", \"||\", \"!\", \"&\", \"|\"]\n",
    "\n",
    "python_keywords = [\"def\", \"class\", \"return\", \"if\", \"else\", \"for\", \"while\", \"in\", \"import\", \"from\", \"as\", \"with\", \"try\", \"except\", \"finally\", \"raise\", \"assert\", \"yield\", \"lambda\", \"pass\", \"break\", \"continue\", \"del\", \"global\", \"nonlocal\", \"and\", \"or\", \"not\", \"is\", \"in\", \"True\", \"False\", \"None\", \"async\", \"await\"]\n",
    "cpp_keywords = [\"class\", \"bool\", \"catch\", \"try\", \"break\", \"continue\", \"delete\", \"do\", \"else\", \"enum\", \"explicit\", \"export\", \"extern\", \"false\", \"for\", \"friend\", \"goto\", \"if\", \"inline\", \"mutable\", \"namespace\", \"new\", \"operator\", \"private\", \"protected\", \"public\", \"register\", \"return\", \"sizeof\", \"static\", \"struct\", \"switch\", \"template\", \"this\", \"throw\", \"true\", \"typedef\", \"typeid\", \"typename\", \"union\", \"using\", \"virtual\", \"volatile\", \"while\"]\n",
    "csharp_keywords = [\"class\", \"bool\", \"catch\", \"try\", \"break\", \"continue\", \"delete\", \"do\", \"else\", \"enum\", \"explicit\", \"export\", \"extern\", \"false\", \"for\", \"friend\", \"goto\", \"if\", \"inline\", \"mutable\", \"namespace\", \"new\", \"operator\", \"private\", \"protected\", \"public\", \"register\", \"return\", \"sizeof\", \"static\", \"struct\", \"switch\", \"template\", \"this\", \"throw\", \"true\", \"typedef\", \"typeid\", \"typename\", \"union\", \"using\", \"virtual\", \"volatile\", \"while\"]\n",
    "go_keywords = [\"break\", \"func\", \"default\", \"type\", \"defer\", \"go\", \"struct\", \"map\", \"chan\", \"else\", \"goto\", \"package\", \"range\", \"const\", \"fallthrough\", \"for\", \"import\", \"interface\", \"return\", \"select\", \"case\", \"continue\", \"if\", \"switch\", \"var\", \"nil\", \"true\", \"false\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cloud(model: Word2Vec, words):\n",
    "    words = [word for word in words if word in model.wv.key_to_index.keys()]\n",
    "    word_vectors = np.array([model.wv.get_normed_vectors()[model.wv.key_to_index[word]] for word in words])\n",
    "    pca = PCA(n_components=2)\n",
    "    result = pca.fit_transform(word_vectors)\n",
    "\n",
    "    df = pd.DataFrame(result, columns=[\"x\", \"y\"])\n",
    "    df[\"model\"] = [word.split(\"_\")[0] if len(word.split(\"_\")) == 2 else \"\" for word in words]\n",
    "    ax = sns.scatterplot(data=df, x=\"x\", y=\"y\", hue=\"model\")\n",
    "    range_x, range_y = np.ptp(df[[\"x\", \"y\"]], axis=0)\n",
    "    keywords = [word.split(\"_\")[1] if len(word.split(\"_\")) == 2 else word for word in words]\n",
    "    for i, word in enumerate(keywords):\n",
    "        x, y = result[i]\n",
    "        ax.text(x + 0.005 * range_x, y, word, verticalalignment='center', horizontalalignment='left', fontsize=8)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud(complete_model, [f\"{language}_{w}\" for w in common_keywords for language in tokenized_sample.language.unique()] + [\"foreach\", 'dataclass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_model.wv.most_similar(positive=[\"np\", \".\", \"array\", \"Go\", \"VARIABLE\"], negative=[\"Python\", \"C++\", \"C#\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_codes = tokenized_sample[tokenized_sample[\"language\"] == 'Python']\n",
    "\n",
    "python_model = FastText(vector_size=100, window=5, min_count=10, workers=8, sg=1)\n",
    "python_model.build_vocab(python_codes.code)\n",
    "python_model.train(python_codes.code, total_examples=python_model.corpus_count, epochs=5)\n",
    "python_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_code_example = \"\"\"#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    query = 7\n",
    "    context = [3, 4, 10, 2, 8, 6, 5, 9, 7, 1]\n",
    "    for i in range(len(context)):\n",
    "        if context[i] == query:\n",
    "            print(f'Query found at index {i}')\n",
    "            os.exit(0)    \n",
    "\n",
    "    print(f'Coulnd't find query {query}')\n",
    "    os.exit(-1)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "}\"\"\"\n",
    "\n",
    "tokenized_code_example = tokenize_document(python_code_example, \"Python\")\n",
    "for word in tokenized_code_example:\n",
    "    print(f' {word}', end=\"\")\n",
    "\n",
    "def next_similar_token(token: str) -> str:\n",
    "    next_most_similar_token, _ = complete_model.wv.most_similar(positive=[token])[0]\n",
    "    return next_most_similar_token\n",
    "\n",
    "\n",
    "\n",
    "next_similar_document = ' '.join([next_similar_token(t) for t in tokenized_code_example])\n",
    "next_similar_document = next_similar_document.replace(':', ':\\n')\n",
    "print(next_similar_document)\n",
    "\n",
    "complete_model.wv.most_similar(positive=['import', 'C++'], negative=['Python'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_tokens = ['import', 'os']\n",
    "\n",
    "#python_model.wv.most_similar_to_given\n",
    "\n",
    "for i in range(20):\n",
    "    next_tokens = python_model.predict_output_word(recent_tokens[-5:])\n",
    "\n",
    "    token_found = False\n",
    "    for t, _ in next_tokens:\n",
    "        if t in recent_tokens[-5:] and t not in ['import', ',']:\n",
    "            continue\n",
    "\n",
    "        recent_tokens.append(t)\n",
    "        print(t, end=' ')\n",
    "        token_found = True\n",
    "\n",
    "    if not token_found:\n",
    "        t, _ = next_tokens[0]\n",
    "        recent_tokens.append(t)\n",
    "        print(t, end=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"red\", \"blue\", \"green\", \"black\"]\n",
    "\n",
    "for language in tokenized_sample.language.unique():\n",
    "    code_for_language = tokenized_sample[tokenized_sample[\"language\"] == language]\n",
    "    token_frequency = {}\n",
    "    for code in code_for_language.code:\n",
    "        for token in code:\n",
    "            token_frequency[token] = token_frequency.get(token, 0) + 1\n",
    "    print(f'Number to different tokens: {len(token_frequency)}')\n",
    "\n",
    "    token_frequency_meta = {}\n",
    "    for m in [10,20,30,40,50,60,70,80,90,100, 120, 140, 160, 180, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 1000]:\n",
    "        for token, freq in token_frequency.items():\n",
    "            if freq >= m:\n",
    "                token_frequency_meta[m] = token_frequency_meta.get(m, 0) + 1\n",
    "    plt.plot(list(token_frequency_meta.keys()), list(token_frequency_meta.values()), label=language)\n",
    "\n",
    "plt.title('Vocabulary size by minimum token frequency')\n",
    "plt.xlabel('Minimum token frequency')\n",
    "plt.ylabel('Vocabulary size')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('programming-languages-w2v-9GOOA69h-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8a0820a20debdd4a590969a108e72dcb302c6f6b612f29f019f671202002b20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
